# model algorithm
model_name: textcnn  # to decide which ml algorithm to be used, example: 'mlp', 'textcnn'

# dataset parameters
dataset: cr_sents  # the dataset name, example: 'cr_sents'
dataset_path: /Users/caiq/Workspace/adp/text-classifier/data/datasets/cr_sents  # folder containing train.csv and test.csv
output_path: /Users/caiq/Workspace/adp/text-classifier/data/outputs/cr_sents/sents  # folder with data files saved by preprocess.py

# preprocess parameters
word_limit: 200 # Truncate long sentences to the limit maximum number of word
min_word_count: 5 # Discard rare words which occur fewer times than this number

# word embeddings parameters
encoder_name:
emb_pretrain: True  # false: initialize embedding weights randomly
                    # true: load pre-trained word embeddings
emb_folder: /Users/caiq/data/vectors/glove.6B  # only makes sense when `emb_pretrain: True`
emb_filename: glove.6B.50d.txt  # only makes sense when `emb_pretrain: True`
emb_size: 256  # word embedding size
               # only makes sense when `emb_pretrain: False`
fine_tune_word_embeddings: True  # fine-tune word embeddings?

# model parameters
hidden_size: 10
n_kernels: 100
kernel_sizes: [3, 4, 5]
n_channels: 2

# checkpoint saving parameters
checkpoint_path: /Users/caiq/Workspace/adp/text-classifier/data/datasets/models/cr_sents/MLP/checkpoints  # path to save checkpoints, null if never save checkpoints

# training parameters
start_epoch: 0  # start at this epoch
batch_size: 64  # batch size
lr: 0.001  # learning rate
lr_decay: 0.9  # a factor to multiply learning rate with (0, 1)
workers: 4  # number of workers for loading data in the DataLoader
num_epochs: 10  # number of epochs to run
grad_clip: null  # clip gradients at this value, null if never clip gradients
print_freq: 100  # print training status every __ batches
checkpoint: null  # path to model checkpoint, null if none
